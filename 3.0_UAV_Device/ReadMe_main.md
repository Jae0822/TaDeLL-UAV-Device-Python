To run this PG-ELLA, run main.py.

具体结果可以参见程序记录.md



```
main.py: the main function to execute
env.py: the definition of a task
associations.py: to generate task samples and a pg-rl function
PGELLA.py: the model of PG-ELLA 
```



```
main.py:
This main script includes many functions for different purposes:
#1. Task Generation:
tasks_gen()     [simplest way to generate tasks]
main()          [test random task generation function: tasks_random(ntasks)]
mainx()         [test special tasks]
mainy()         [To test and generate qualified tasks]

#2: Debug Basic Functions
main0()         [function djd_nac and get_hessian is proved to be correct]
main1()         [Test the change of L and A and b when model.update_model is called by the same task repeatedly]
main2()         [Test hessian matrix]

#3: Different Training modes (All failed)
mainA1()        [Prepare the pg_rl of all tasks for mainA2]
mainA2()        [Training Mode A: 50 * 30 + 30 * n]
mainB()         [Training Mode B: 50 * 30 * n]
mainC1()        [Training Mode C: 50 * 30 * 1 or 50 * 30 * n]
mainC2()        [This function is used to compute regular policy gradient and obtain values for tasks used to train in mainC1()]
mainC3()        [This function is used to plot and compare results of pg-Ella and regular pg_rl.]
testing()       [This function is used to test the performance of PG-ELLA.]

#4: Final Success!
pg_ella()       [This function is used to simulate PG-ELLA algorithm as in Matlab: https://github.com/haithamb/pginterella]
```



```
env.py: 
This script inlcudes the definition of class Task.
init() [initilize of the class, includes device specific, task specific, model specific parameters]

#1: MDP Related
reset(self)                         [reset time and pending_packets]
draw_init_state(self)               [Follows a Bernoulli distribution]
draw_action(self,state,alpha)       [Gaussian Policy: action = policy['theta'] * state + policy['sigma']]
step(self, state, _action)          [step given state, action and policy]
collect_path(self, alpha)           [collect path given policy]
get_value(self, alpha)              [get value of a path given policy]

#2: Policy Gradient Related
djd_nac(self, path)                 [Compute a policy gradient step based on Natural Actor Critic]
gaussian_grad(self, state, action)  [x][Compute the gradient of the gaussian policy](Not used)
get_hessian(self, alpha)            [Compute Hessian Matrix of Gaussian Policy given a path (alpha/policy)]

# Tips: [x]: Unuseful.
```



```
associations.py:

#1: Learning Related
pg_rl(task, niter=50, lr=0.08)       [execute the regular policy gradient]

#2: Tasks Generation Realted
tasks_lib_normal()                   [x][manually generate tasks]
tasks_lib_special()                  [x][manually generate special tasks that can't be learned by lr = 0.1]
tasks_random(ntask)                  [*][generate a set of tasks and test looply, until all tasks are qualified] 
task_replace(tasks, x)               [when an abnormal task appears, generate a new one]

# Tips: tasks_random.pkl is generated by [*] and used in other functions. lr = 0.08
# Tips: [x]: unuseful
```



```
PGELLA.py:
Definition of Class Model

__init__()                                 [initlization of class Model]
update_model(self, task, alpha, hessian)   [update model given a task and its alpha, hessian]

```

